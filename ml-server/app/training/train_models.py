"""
Model Training Script for Resumelyze ML Server

Trains three models:
  1. ATS Scorer — GradientBoosting on 20 handcrafted features
  2. Section Scorer — GradientBoosting on 12 features per section
  3. Overall Grader — GradientBoosting on 9 composite features

Each model falls back to rule-based scoring if not trained yet.
Training uses synthetic data generated by generate_data.py and
real ML features extracted via the model modules.

Target: 95%+ accuracy (MAE < 5 on 0-100 scale)
"""

import json
import os
import sys
import time
import numpy as np

# Fix imports
sys.path.insert(0, os.path.join(os.path.dirname(__file__), "..", ".."))

from sklearn.ensemble import GradientBoostingRegressor, GradientBoostingClassifier
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.metrics import mean_absolute_error, r2_score, accuracy_score
from sklearn.preprocessing import LabelEncoder
import joblib

from app.config import MODELS_DIR, DATA_DIR, GRADE_MAP


def load_training_data():
    """Load the unified training data (synthetic + real + feedback)."""
    # Try unified dataset first
    unified_path = os.path.join(DATA_DIR, "unified_dataset.json")
    if os.path.exists(unified_path):
        with open(unified_path, "r") as f:
            data = json.load(f)
        print(f"  Loaded unified dataset ({len(data)} samples)")
        return data

    # Fall back to synthetic-only
    path = os.path.join(DATA_DIR, "training_data.json")
    if not os.path.exists(path):
        print("[!] No training data found. Generating synthetic data...")
        from app.training.generate_data import generate_dataset
        generate_dataset(5000)

    with open(path, "r") as f:
        data = json.load(f)
    print(f"  Loaded synthetic dataset ({len(data)} samples)")
    return data


def extract_features_for_ats(sample: dict) -> np.ndarray:
    """
    Extract ATS features from a sample.
    Uses the real ML feature extraction pipeline.
    """
    from app.models.nlp_engine import (
        detect_sections, detect_contact_info,
        compute_keyword_overlap, extract_keywords,
        count_bullet_points, compute_readability,
    )
    from app.models.ats_scorer import extract_ats_features

    resume = sample["resume"]
    jd = sample["job_description"]
    sections = detect_sections(resume)

    features = extract_ats_features(resume, jd, sections)
    return np.array(features)


def extract_features_for_section(section_text: str, jd: str, name: str, sem_sim: float) -> np.ndarray:
    """Extract section features for section scorer training."""
    from app.models.section_scorer import extract_section_features
    return np.array(extract_section_features(section_text, jd, name, sem_sim))


def extract_features_for_grade(sample: dict) -> np.ndarray:
    """
    Extract composite features for grader training.
    """
    quality = sample["quality"]
    noise = np.random.normal(0, 3)

    # Simulate feature values based on quality
    jd_match = min(100, max(0, quality * 85 + 10 + noise))
    ats_score = min(100, max(0, quality * 80 + 15 + np.random.normal(0, 4)))
    avg_section = min(100, max(0, quality * 75 + 15 + np.random.normal(0, 5)))
    readability = min(100, max(0, quality * 60 + 30 + np.random.normal(0, 8)))
    verb_score = min(100, max(0, quality * 80 + np.random.normal(0, 5)))
    quant_score = min(100, max(0, quality * 70 + np.random.normal(0, 8)))
    cliche_penalty = max(0, (1 - quality) * 30 + np.random.normal(0, 3))
    kw_density = min(1, max(0, quality * 0.6 + 0.1 + np.random.normal(0, 0.05)))
    completeness = quality * 0.8 + 0.2 + np.random.normal(0, 0.05)

    return np.array([
        jd_match, ats_score, avg_section, readability,
        verb_score, quant_score, cliche_penalty, kw_density,
        completeness,
    ])


def score_to_grade(score: float) -> str:
    """Convert numeric score to letter grade."""
    for threshold, grade in GRADE_MAP:
        if score >= threshold:
            return grade
    return "F"


def train_ats_model(data: list):
    """Train ATS scorer model."""
    print("\n" + "=" * 60)
    print("  Training ATS Scorer Model")
    print("=" * 60)

    # Subsample for speed during feature extraction (NLP operations are slow)
    subset = data[:2000] if len(data) > 2000 else data

    print(f"  Extracting features from {len(subset)} samples...")
    t0 = time.time()

    X, y = [], []
    for i, sample in enumerate(subset):
        try:
            features = extract_features_for_ats(sample)
            X.append(features)
            y.append(sample["expected_ats_score"])

            if (i + 1) % 200 == 0:
                print(f"    Processed {i + 1}/{len(subset)}...")
        except Exception as e:
            continue

    X = np.array(X)
    y = np.array(y)
    print(f"  Feature extraction done in {time.time() - t0:.1f}s ({X.shape[0]} samples, {X.shape[1]} features)")

    # Split
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    # Train
    model = GradientBoostingRegressor(
        n_estimators=200,
        max_depth=5,
        learning_rate=0.1,
        subsample=0.8,
        random_state=42,
    )

    print("  Training GradientBoosting...")
    model.fit(X_train, y_train)

    # Evaluate
    y_pred = model.predict(X_test)
    mae = mean_absolute_error(y_test, y_pred)
    r2 = r2_score(y_test, y_pred)
    accuracy_5 = np.mean(np.abs(y_test - y_pred) <= 5) * 100  # Within 5 points

    print(f"\n  Results:")
    print(f"    MAE: {mae:.2f}")
    print(f"    R²:  {r2:.3f}")
    print(f"    Accuracy (±5): {accuracy_5:.1f}%")

    # Cross-validation
    cv_scores = cross_val_score(model, X, y, cv=5, scoring="neg_mean_absolute_error")
    print(f"    CV MAE: {-cv_scores.mean():.2f} ± {cv_scores.std():.2f}")

    # Save
    os.makedirs(MODELS_DIR, exist_ok=True)
    path = os.path.join(MODELS_DIR, "ats_scorer.joblib")
    joblib.dump(model, path)
    print(f"  [OK] Saved to {path}")

    return model, {"mae": mae, "r2": r2, "accuracy_5": accuracy_5}


def train_section_model(data: list):
    """Train section quality scorer model."""
    print("\n" + "=" * 60)
    print("  Training Section Scorer Model")
    print("=" * 60)

    from app.models.nlp_engine import detect_sections
    from app.models.semantic import compute_section_similarities

    subset = data[:1500] if len(data) > 1500 else data

    print(f"  Extracting section features from {len(subset)} samples...")
    t0 = time.time()

    X, y = [], []
    section_names = ["summary", "skills", "experience", "education", "projects"]

    for i, sample in enumerate(subset):
        try:
            resume = sample["resume"]
            jd = sample["job_description"]
            quality = sample["quality"]
            sections = detect_sections(resume)

            try:
                sims = compute_section_similarities(sections, jd)
            except Exception:
                sims = {}

            for name in section_names:
                text = sections.get(name, "")
                sem_sim = sims.get(name, 0.0)
                features = extract_features_for_section(text, jd, name, sem_sim)
                X.append(features)

                # Label: quality-based + noise
                noise = np.random.normal(0, 5)
                if text:
                    section_score = min(100, max(0, quality * 80 + 10 + noise))
                else:
                    section_score = max(0, min(30, noise + 10))
                y.append(section_score)

            if (i + 1) % 200 == 0:
                print(f"    Processed {i + 1}/{len(subset)}...")

        except Exception:
            continue

    X = np.array(X)
    y = np.array(y)
    print(f"  Done in {time.time() - t0:.1f}s ({X.shape[0]} samples, {X.shape[1]} features)")

    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    model = GradientBoostingRegressor(
        n_estimators=150,
        max_depth=4,
        learning_rate=0.1,
        subsample=0.8,
        random_state=42,
    )

    print("  Training GradientBoosting...")
    model.fit(X_train, y_train)

    y_pred = model.predict(X_test)
    mae = mean_absolute_error(y_test, y_pred)
    r2 = r2_score(y_test, y_pred)
    accuracy_5 = np.mean(np.abs(y_test - y_pred) <= 5) * 100

    print(f"\n  Results:")
    print(f"    MAE: {mae:.2f}")
    print(f"    R²:  {r2:.3f}")
    print(f"    Accuracy (±5): {accuracy_5:.1f}%")

    path = os.path.join(MODELS_DIR, "section_scorer.joblib")
    joblib.dump(model, path)
    print(f"  [OK] Saved to {path}")

    return model, {"mae": mae, "r2": r2, "accuracy_5": accuracy_5}


def train_grade_model(data: list):
    """Train overall grading model (regression + classification)."""
    print("\n" + "=" * 60)
    print("  Training Grade Model")
    print("=" * 60)

    print(f"  Extracting grade features from {len(data)} samples...")
    t0 = time.time()

    X, y_numeric, y_grade = [], [], []

    for sample in data:
        features = extract_features_for_grade(sample)
        X.append(features)

        quality = sample["quality"]
        score = min(100, max(0, quality * 90 + 5 + np.random.normal(0, 3)))
        y_numeric.append(score)
        y_grade.append(score_to_grade(score))

    X = np.array(X)
    y_numeric = np.array(y_numeric)
    y_grade = np.array(y_grade)
    print(f"  Done in {time.time() - t0:.1f}s")

    # ── Regression model (numeric score) ──
    X_train, X_test, y_train, y_test = train_test_split(
        X, y_numeric, test_size=0.2, random_state=42
    )

    reg = GradientBoostingRegressor(
        n_estimators=300,
        max_depth=5,
        learning_rate=0.1,
        subsample=0.8,
        random_state=42,
    )

    print("  Training regression model...")
    reg.fit(X_train, y_train)

    y_pred = reg.predict(X_test)
    mae = mean_absolute_error(y_test, y_pred)
    r2 = r2_score(y_test, y_pred)
    accuracy_5 = np.mean(np.abs(y_test - y_pred) <= 5) * 100

    print(f"\n  Regression Results:")
    print(f"    MAE: {mae:.2f}")
    print(f"    R²:  {r2:.3f}")
    print(f"    Accuracy (±5): {accuracy_5:.1f}%")

    # ── Classification model (letter grade) ──
    le = LabelEncoder()
    y_grade_enc = le.fit_transform(y_grade)

    X_train_c, X_test_c, y_train_c, y_test_c = train_test_split(
        X, y_grade_enc, test_size=0.2, random_state=42
    )

    clf = GradientBoostingClassifier(
        n_estimators=200,
        max_depth=4,
        learning_rate=0.1,
        random_state=42,
    )

    print("  Training classification model...")
    clf.fit(X_train_c, y_train_c)

    y_pred_c = clf.predict(X_test_c)
    grade_accuracy = accuracy_score(y_test_c, y_pred_c)
    # "Close" accuracy: within 1 grade
    close_acc = np.mean(np.abs(y_test_c - y_pred_c) <= 1) * 100

    print(f"\n  Grade Classification:")
    print(f"    Exact accuracy: {grade_accuracy * 100:.1f}%")
    print(f"    ±1 grade accuracy: {close_acc:.1f}%")

    # Save
    os.makedirs(MODELS_DIR, exist_ok=True)
    path_reg = os.path.join(MODELS_DIR, "grader_regressor.joblib")
    path_clf = os.path.join(MODELS_DIR, "grader_classifier.joblib")
    path_le = os.path.join(MODELS_DIR, "grader_label_encoder.joblib")

    joblib.dump(reg, path_reg)
    joblib.dump(clf, path_clf)
    joblib.dump(le, path_le)
    print(f"  [OK] Saved to {MODELS_DIR}")

    return {
        "regressor": reg,
        "classifier": clf,
        "label_encoder": le,
        "metrics": {
            "mae": mae, "r2": r2, "accuracy_5": accuracy_5,
            "grade_accuracy": grade_accuracy, "close_accuracy": close_acc,
        }
    }


def train_all():
    """Train all models end-to-end."""
    print("\n" + "=" * 50)
    print("  Resumelyze ML -- Full Training Pipeline")
    print("=" * 50 + "\n")

    t0 = time.time()

    # Load data
    print("[1/4] Loading training data...")
    data = load_training_data()
    print(f"  Loaded {len(data)} samples\n")

    # Train models
    print("[2/4] Training ATS Scorer...")
    ats_model, ats_metrics = train_ats_model(data)

    print("\n[3/4] Training Section Scorer...")
    section_model, section_metrics = train_section_model(data)

    print("\n[4/4] Training Grader...")
    grade_result = train_grade_model(data)

    # Summary
    elapsed = time.time() - t0
    print("\n" + "=" * 60)
    print("  Training Complete!")
    print("=" * 60)
    print(f"  Total time: {elapsed / 60:.1f} minutes")
    print(f"\n  Model Performance Summary:")
    print(f"  {'Model':<20} {'MAE':<8} {'R²':<8} {'Acc (±5)':<10}")
    print(f"  {'-' * 46}")
    print(f"  {'ATS Scorer':<20} {ats_metrics['mae']:<8.2f} {ats_metrics['r2']:<8.3f} {ats_metrics['accuracy_5']:<10.1f}%")
    print(f"  {'Section Scorer':<20} {section_metrics['mae']:<8.2f} {section_metrics['r2']:<8.3f} {section_metrics['accuracy_5']:<10.1f}%")
    gm = grade_result['metrics']
    print(f"  {'Grader (reg)':<20} {gm['mae']:<8.2f} {gm['r2']:<8.3f} {gm['accuracy_5']:<10.1f}%")
    print(f"  {'Grader (clf)':<20} {'-':<8} {'-':<8} {gm['grade_accuracy'] * 100:<10.1f}%")
    print()

    return {
        "ats": ats_metrics,
        "section": section_metrics,
        "grade": grade_result["metrics"],
    }


if __name__ == "__main__":
    train_all()
